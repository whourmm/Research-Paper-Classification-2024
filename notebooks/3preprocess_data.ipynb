{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to D:/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to D:/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to D:/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import io\n",
    "import threading\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk_data_path = \"D:/nltk_data\"  # Change this to your desired directory\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)\n",
    "\n",
    "# Append the path to NLTK's data search paths\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Download the required NLTK data to the custom path\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.data.path.append(\"D:/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>categories/keyword</th>\n",
       "      <th>author_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>childhood trauma and childhood mental disorder...</td>\n",
       "      <td>journal of nervous and mental disease</td>\n",
       "      <td>['childhood mental disorder', 'childhood traum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>morphological plasticity of hyperelongated cel...</td>\n",
       "      <td>applied microbiology and biotechnology</td>\n",
       "      <td>['cyanobacteria', 'elongation factor p', 'hype...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  childhood trauma and childhood mental disorder...   \n",
       "1  morphological plasticity of hyperelongated cel...   \n",
       "\n",
       "                       categories/keyword  \\\n",
       "0   journal of nervous and mental disease   \n",
       "1  applied microbiology and biotechnology   \n",
       "\n",
       "                                         author_tags  \n",
       "0  ['childhood mental disorder', 'childhood traum...  \n",
       "1  ['cyanobacteria', 'elongation factor p', 'hype...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Remove HTML tags, special characters, and punctuation.\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and punctuation\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens): #running -> run / better -> good\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def tokenize_text(text): # text = \"The dogs are running fast.\" -> Output: ['The', 'dogs', 'are', 'running', 'fast', '.']\n",
    "    tokens = word_tokenize(text, language='english', preserve_line=True)\n",
    "    return tokens\n",
    "\n",
    "def handle_missing_data(text, placeholder='Missing'): # Input = \" \" -> #Output: \"Missing\"\n",
    "    \"\"\"Handle missing or noisy data.\"\"\"\n",
    "    if pd.isnull(text) or text.strip() == \"\":\n",
    "        return placeholder\n",
    "    return text\n",
    "\n",
    "def to_lowercase(tokens):\n",
    "    \"\"\"Convert tokens to lowercase.\"\"\"\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens): # remove a, an, the, this, etc.\n",
    "    \"\"\"Remove stopwords.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Complete text preprocessing pipeline.\"\"\"\n",
    "    text = handle_missing_data(text)  # Handle missing or noisy data\n",
    "    text = clean_text(text)          # Clean text (remove unwanted characters)\n",
    "    tokens = tokenize_text(text)     # Tokenize text\n",
    "    tokens = to_lowercase(tokens)    # Convert to lowercase\n",
    "    tokens = remove_stopwords(tokens)  # Remove stopwords\n",
    "    tokens = lemmatize_tokens(tokens)  # Or stem_tokens(tokens) for stemming\n",
    "    return ' '.join(tokens)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker = 'localhost:29092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10 (consume_messages):\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\dsde-cedt\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"d:\\anaconda3\\envs\\dsde-cedt\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22720\\3434032373.py\", line 2, in consume_messages\n",
      "  File \"d:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\kafka\\consumer\\group.py\", line 356, in __init__\n",
      "    self._client = KafkaClient(metrics=self._metrics, **self.config)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\kafka\\client_async.py\", line 244, in __init__\n",
      "    self.config['api_version'] = self.check_version(timeout=check_timeout)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\kafka\\client_async.py\", line 927, in check_version\n",
      "    raise Errors.NoBrokersAvailable()\n",
      "kafka.errors.NoBrokersAvailable: NoBrokersAvailable\n"
     ]
    }
   ],
   "source": [
    "def consume_messages():\n",
    "    consumer = KafkaConsumer(\n",
    "        'processed_data',\n",
    "        bootstrap_servers=[kafka_broker],\n",
    "        enable_auto_commit=True,\n",
    "        value_deserializer=lambda x: x.decode('utf-8')\n",
    "    )\n",
    "    output_file_path = 'preprocess_data.csv'\n",
    "    print('Running Consumer')\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[kafka_broker],\n",
    "        linger_ms=5000,\n",
    "        acks='all',\n",
    "        max_block_ms=60000\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for message in consumer:\n",
    "            print(f\"Received message: [{message.timestamp}:{message.offset}] {message.value}\")\n",
    "            \n",
    "            # Write the received message (CSV content) to a file\n",
    "            with open(output_file_path, 'w') as f:\n",
    "                f.write(message.value)  # Save the content to the file\n",
    "\n",
    "            print(f\"Message saved to {output_file_path}\")\n",
    "            \n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(output_file_path)\n",
    "            \n",
    "            # Data cleaning steps\n",
    "            df['title'] = df['title'].apply(preprocess_text)\n",
    "            df['categories/keyword'] = df['categories/keyword'].apply(preprocess_text)\n",
    "            df['author_tags'] = df['author_tags'].apply(preprocess_text)\n",
    "            # Save the cleaned DataFrame\n",
    "            \n",
    "            # Save cleaned data to a new CSV\n",
    "            df.to_csv(output_file_path , index=False)\n",
    "            producer.send('response_from_3', \"Success\")  # Send the cleaned CSV content back\n",
    "            time.sleep(2)  # Simulate some delay\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while consuming: {e}\")\n",
    "\n",
    "# Producer function\n",
    "# def produce_messages():\n",
    "#     csv_file_path = 'preprocess_data.csv'\n",
    "#     producer = KafkaProducer(\n",
    "#         bootstrap_servers=[kafka_broker],\n",
    "#         linger_ms=5000,\n",
    "#         acks='all',\n",
    "#         max_block_ms=60000\n",
    "#     )\n",
    "#     while not os.path.exists(csv_file_path):\n",
    "#         print(f\"Waiting for {csv_file_path} to appear...\")\n",
    "#         time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "\n",
    "#     print(f\"{csv_file_path} found! Proceeding to send data...\")\n",
    "#     with open(csv_file_path, 'r') as csvfile:\n",
    "#         csv_content = csvfile.read()  # Read entire file content\n",
    "\n",
    "#     # Send the entire CSV content to Kafka as a single message\n",
    "#     print(f'Sending entire CSV content to Kafka: {csv_content[:100]}...')  # Display first 100 chars for logging\n",
    "#     producer.send('preprocess_data', csv_content.encode('utf-8'))  # Send the CSV content\n",
    "#     time.sleep(2)\n",
    "\n",
    "\n",
    "    # Ensure all messages are sent before exiting\n",
    "    producer.flush()\n",
    "\n",
    "# Running producer and consumer in separate threads\n",
    "# producer_thread = threading.Thread(target=produce_messages)\n",
    "consumer_thread = threading.Thread(target=consume_messages)\n",
    "\n",
    "# producer_thread.start()\n",
    "consumer_thread.start()\n",
    "\n",
    "# producer_thread.join()\n",
    "consumer_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/2018_cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/2018_cleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/2018_cleaned.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/2018_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: applying, TF-IDF Score: 0.3403172942934855\n",
      "Word: caregiver, TF-IDF Score: 0.3179765854455264\n",
      "Word: case, TF-IDF Score: 0.21998024295224758\n",
      "Word: distance, TF-IDF Score: 0.3179765854455264\n",
      "Word: elderlyarticle, TF-IDF Score: 0.35805011251327074\n",
      "Word: internationalization, TF-IDF Score: 0.35805011251327074\n",
      "Word: japanese, TF-IDF Score: 0.327735659539065\n",
      "Word: psychic, TF-IDF Score: 0.35805011251327074\n",
      "Word: service, TF-IDF Score: 0.27294668374665065\n",
      "Word: study, TF-IDF Score: 0.18609364123438332\n",
      "Word: thai, TF-IDF Score: 0.1800932172800236\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer_titles = TfidfVectorizer(max_features=10000, min_df=1, max_df=0.8)\n",
    "df['title_tfidf'] = list(tfidf_vectorizer_titles.fit_transform(df['title']).toarray())\n",
    "\n",
    "# Sentence Embeddings for abstracts\n",
    "# df['tags_tfidf'] = list(tfidf_vectorizer.fit_transform(df['author_tags']).toarray())\n",
    "\n",
    "# Save extracted features\n",
    "df.to_pickle(\"processed_features.pkl\")\n",
    "# df['author_tags'].head()\n",
    "# df[['title_tfidf', 'tags_tfidf']].head()\n",
    "# Get the feature names (words) from the vectorizer\n",
    "vocab = tfidf_vectorizer_titles.get_feature_names_out()\n",
    "title_tfidf_vector = df['title_tfidf'][1]  # TF-IDF vector\n",
    "nonzero_indices = [i for i, value in enumerate(title_tfidf_vector) if value != 0]\n",
    "\n",
    "# Print words and their corresponding TF-IDF scores\n",
    "for index in nonzero_indices:\n",
    "    print(f\"Word: {vocab[index]}, TF-IDF Score: {title_tfidf_vector[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to specific columns (e.g., 'title' and 'abstract')\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories/keyword\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories/keyword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor_tags\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor_tags\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to specific columns (e.g., 'title' and 'abstract')\n",
    "df['title'] = df['title'].apply(preprocess_text)\n",
    "df['categories/keyword'] = df['categories/keyword'].apply(preprocess_text)\n",
    "df['author_tags'] = df['author_tags'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "df.to_csv('data/2018_cleaned_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>categories/keyword</th>\n",
       "      <th>author_tags</th>\n",
       "      <th>title_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hydrogeological parameter distribution estimat...</td>\n",
       "      <td>international journal civil engineering techno...</td>\n",
       "      <td>groundwater model hydraulic conductivity krigi...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>applying psychic distance service internationa...</td>\n",
       "      <td>journal asiapacific business</td>\n",
       "      <td>cultural distance health care service japan pe...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>estimation aloeemodin content cassia grandis c...</td>\n",
       "      <td>indian journal pharmaceutical science</td>\n",
       "      <td>aloeemodin content cassia garrettiana cassia g...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antirice pathogenic microbial activity persica...</td>\n",
       "      <td>science technology asia</td>\n",
       "      <td>antimicrobial activity essential oil persicari...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line orf protein upregulated reactive oxygen s...</td>\n",
       "      <td>cancer genomics proteomics</td>\n",
       "      <td>hne bladder cancer cancer progression immunohi...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  hydrogeological parameter distribution estimat...   \n",
       "1  applying psychic distance service internationa...   \n",
       "2  estimation aloeemodin content cassia grandis c...   \n",
       "3  antirice pathogenic microbial activity persica...   \n",
       "4  line orf protein upregulated reactive oxygen s...   \n",
       "\n",
       "                                  categories/keyword  \\\n",
       "0  international journal civil engineering techno...   \n",
       "1                       journal asiapacific business   \n",
       "2              indian journal pharmaceutical science   \n",
       "3                            science technology asia   \n",
       "4                         cancer genomics proteomics   \n",
       "\n",
       "                                         author_tags              title_tfidf  \n",
       "0  groundwater model hydraulic conductivity krigi...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "1  cultural distance health care service japan pe...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "2  aloeemodin content cassia garrettiana cassia g...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "3  antimicrobial activity essential oil persicari...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "4  hne bladder cancer cancer progression immunohi...  [0. 0. 0. ... 0. 0. 0.]  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.read_csv('data/2018_cleaned_processed.csv')\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>categories/keyword</th>\n",
       "      <th>author_tags</th>\n",
       "      <th>access</th>\n",
       "      <th>acid</th>\n",
       "      <th>activity</th>\n",
       "      <th>acute</th>\n",
       "      <th>among</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>...</th>\n",
       "      <th>thailandarticle</th>\n",
       "      <th>thailandarticleopen</th>\n",
       "      <th>therapy</th>\n",
       "      <th>treatment</th>\n",
       "      <th>two</th>\n",
       "      <th>use</th>\n",
       "      <th>using</th>\n",
       "      <th>via</th>\n",
       "      <th>virus</th>\n",
       "      <th>water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hydrogeological parameter distribution estimat...</td>\n",
       "      <td>international journal civil engineering techno...</td>\n",
       "      <td>groundwater model hydraulic conductivity krigi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>applying psychic distance service internationa...</td>\n",
       "      <td>journal asiapacific business</td>\n",
       "      <td>cultural distance health care service japan pe...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>estimation aloeemodin content cassia grandis c...</td>\n",
       "      <td>indian journal pharmaceutical science</td>\n",
       "      <td>aloeemodin content cassia garrettiana cassia g...</td>\n",
       "      <td>0.280471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antirice pathogenic microbial activity persica...</td>\n",
       "      <td>science technology asia</td>\n",
       "      <td>antimicrobial activity essential oil persicari...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line orf protein upregulated reactive oxygen s...</td>\n",
       "      <td>cancer genomics proteomics</td>\n",
       "      <td>hne bladder cancer cancer progression immunohi...</td>\n",
       "      <td>0.207590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  hydrogeological parameter distribution estimat...   \n",
       "1  applying psychic distance service internationa...   \n",
       "2  estimation aloeemodin content cassia grandis c...   \n",
       "3  antirice pathogenic microbial activity persica...   \n",
       "4  line orf protein upregulated reactive oxygen s...   \n",
       "\n",
       "                                  categories/keyword  \\\n",
       "0  international journal civil engineering techno...   \n",
       "1                       journal asiapacific business   \n",
       "2              indian journal pharmaceutical science   \n",
       "3                            science technology asia   \n",
       "4                         cancer genomics proteomics   \n",
       "\n",
       "                                         author_tags    access  acid  \\\n",
       "0  groundwater model hydraulic conductivity krigi...  0.000000   0.0   \n",
       "1  cultural distance health care service japan pe...  0.000000   0.0   \n",
       "2  aloeemodin content cassia garrettiana cassia g...  0.280471   0.0   \n",
       "3  antimicrobial activity essential oil persicari...  0.000000   0.0   \n",
       "4  hne bladder cancer cancer progression immunohi...  0.207590   0.0   \n",
       "\n",
       "   activity  acute  among  analysis  application  ...  thailandarticle  \\\n",
       "0       0.0    0.0    0.0       0.0          0.0  ...         0.634454   \n",
       "1       0.0    0.0    0.0       0.0          0.0  ...         0.000000   \n",
       "2       0.0    0.0    0.0       0.0          0.0  ...         0.000000   \n",
       "3       1.0    0.0    0.0       0.0          0.0  ...         0.000000   \n",
       "4       0.0    0.0    0.0       0.0          0.0  ...         0.000000   \n",
       "\n",
       "   thailandarticleopen  therapy  treatment  two  use     using  via  virus  \\\n",
       "0                  0.0      0.0        0.0  0.0  0.0  0.000000  0.0    0.0   \n",
       "1                  0.0      0.0        0.0  0.0  0.0  0.000000  0.0    0.0   \n",
       "2                  0.0      0.0        0.0  0.0  0.0  0.540294  0.0    0.0   \n",
       "3                  0.0      0.0        0.0  0.0  0.0  0.000000  0.0    0.0   \n",
       "4                  0.0      0.0        0.0  0.0  0.0  0.000000  0.0    0.0   \n",
       "\n",
       "   water  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example pipeline for feature extraction\n",
    "# TF-IDF for titles\n",
    "tfidf_vectorizer_titles = TfidfVectorizer(max_features=10000, min_df=0.01, max_df=0.8) #exclude > 80% duplicate and < 1%\n",
    "df['title_tfidf']  = list(tfidf_vectorizer_titles.fit_transform(df['title']).toarray())\n",
    "\n",
    "X_tfidf = tfidf_vectorizer_titles.fit_transform(df['title']).toarray()\n",
    "\n",
    "# Get the feature names (the words)\n",
    "header = tfidf_vectorizer_titles.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF array to a DataFrame\n",
    "df_tfidf = pd.DataFrame(X_tfidf, columns=header)\n",
    "\n",
    "# Now you can add this DataFrame back to your original DataFrame\n",
    "df = pd.concat([df, df_tfidf], axis=1)\n",
    "df = df.drop(columns=['title_tfidf'])\n",
    "\n",
    "df.head()\n",
    "# sample_tfidf = tfidf_vectorizer_titles.fit_transform()\n",
    "\n",
    "\n",
    "# # Sentence Embeddings for abstracts\n",
    "# # df['tags_tfidf'] = list(tfidf_vectorizer.fit_transform(df['author_tags']).toarray())\n",
    "\n",
    "# # Save extracted features\n",
    "# df.to_pickle(\"processed_features.pkl\")\n",
    "# # df['author_tags'].head()\n",
    "# # df[['title_tfidf', 'tags_tfidf']].head()\n",
    "\n",
    "# print(name_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_titles = TfidfVectorizer(max_features=5000, min_df=0.01, max_df=0.8)\n",
    "title_tfidf = tfidf_vectorizer_titles.fit_transform(df['title'])\n",
    "\n",
    "# TF-IDF for tags\n",
    "tfidf_vectorizer_tags = TfidfVectorizer(max_features=5000, min_df=0.01, max_df=0.8)\n",
    "tags_tfidf = tfidf_vectorizer_tags.fit_transform(df['author_tags'])\n",
    "\n",
    "# Optionally, get the feature names (terms)\n",
    "title_feature_names = tfidf_vectorizer_titles.get_feature_names_out()\n",
    "tags_feature_names = tfidf_vectorizer_tags.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.28047099, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Combine the separate TF-IDF features into one feature matrix\n",
    "X = hstack([title_tfidf, tags_tfidf])\n",
    "\n",
    "# Convert the combined features to a dense format (optional, depending on model)\n",
    "# X = X.toarray()  # Only if your model requires dense arrays, some models like Logistic Regression do\n",
    "X.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
