{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to D:/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to D:/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to D:/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import io\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk_data_path = \"D:/nltk_data\"  # Change this to your desired directory\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)\n",
    "\n",
    "# Append the path to NLTK's data search paths\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "# Download the required NLTK data to the custom path\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.data.path.append(\"D:/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Remove HTML tags, special characters, and punctuation.\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and punctuation\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens): #running -> run / better -> good\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def tokenize_text(text): # text = \"The dogs are running fast.\" -> Output: ['The', 'dogs', 'are', 'running', 'fast', '.']\n",
    "    tokens = word_tokenize(text, language='english', preserve_line=True)\n",
    "    return tokens\n",
    "\n",
    "def handle_missing_data(text, placeholder='Missing'): # Input = \" \" -> #Output: \"Missing\"\n",
    "    \"\"\"Handle missing or noisy data.\"\"\"\n",
    "    if pd.isnull(text) or text.strip() == \"\":\n",
    "        return placeholder\n",
    "    return text\n",
    "\n",
    "def to_lowercase(tokens):\n",
    "    \"\"\"Convert tokens to lowercase.\"\"\"\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens): # remove a, an, the, this, etc.\n",
    "    \"\"\"Remove stopwords.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Complete text preprocessing pipeline.\"\"\"\n",
    "    text = handle_missing_data(text)  # Handle missing or noisy data\n",
    "    text = clean_text(text)          # Clean text (remove unwanted characters)\n",
    "    tokens = tokenize_text(text)     # Tokenize text\n",
    "    tokens = to_lowercase(tokens)    # Convert to lowercase\n",
    "    tokens = remove_stopwords(tokens)  # Remove stopwords\n",
    "    tokens = lemmatize_tokens(tokens)  # Or stem_tokens(tokens) for stemming\n",
    "    return ' '.join(tokens)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "kafka_broker = 'localhost:29092'\n",
    "consumer = KafkaConsumer(\n",
    "    'processed_data',\n",
    "    bootstrap_servers=[kafka_broker],\n",
    "    enable_auto_commit=True,\n",
    "    value_deserializer=lambda x: x.decode('utf-8')\n",
    ")\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[kafka_broker],\n",
    "    linger_ms=5000,\n",
    "    acks='all',\n",
    "    max_block_ms=60000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for data\n",
      "ConsumerRecord(topic='processed_data', partition=0, offset=113, timestamp=1733422892438, timestamp_type=0, key=None, value='title,categories/keyword,author_tags\\nincidence and risk factors associated with injuries during static line parachute training in royal thai army(article)(open access),military medical research,\"[\\'injuries\\', \\'parachute\\', \\'paratroopers\\', \\'adult\\', \\'aircraft\\', \\'article\\', \\'cohort analysis\\', \\'demography\\', \\'health program\\', \\'helicopter\\', \\'human\\', \\'incidence\\', \\'information processing\\', \\'major clinical study\\', \\'medical service\\', \\'military personnel\\', \\'motion sickness\\', \\'occupational accident\\', \\'parachutist\\', \\'priority journal\\', \\'prospective study\\', \\'risk factor\\', \\'training\\', \\'wind speed\\', \\'young adult\\', \\'adolescent\\', \\'aviation\\', \\'education\\', \\'incidence\\', \\'injury\\', \\'male\\', \\'procedures\\', \\'risk factor\\', \\'teaching\\', \\'thailand\\', \\'adolescent\\', \\'adult\\', \\'aviation\\', \\'cohort studies\\', \\'humans\\', \\'incidence\\', \\'male\\', \\'military personnel\\', \\'prospective studies\\', \\'risk factors\\', \\'teaching\\', \\'thailand\\', \\'wounds and injuries\\']\"\\n', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=919, serialized_header_size=-1)\n",
      "Received message: [1733422892438:113] title,categories/keyword,author_tags\n",
      "incidence and risk factors associated with injuries during static line parachute training in royal thai army(article)(open access),military medical research,\"['injuries', 'parachute', 'paratroopers', 'adult', 'aircraft', 'article', 'cohort analysis', 'demography', 'health program', 'helicopter', 'human', 'incidence', 'information processing', 'major clinical study', 'medical service', 'military personnel', 'motion sickness', 'occupational accident', 'parachutist', 'priority journal', 'prospective study', 'risk factor', 'training', 'wind speed', 'young adult', 'adolescent', 'aviation', 'education', 'incidence', 'injury', 'male', 'procedures', 'risk factor', 'teaching', 'thailand', 'adolescent', 'adult', 'aviation', 'cohort studies', 'humans', 'incidence', 'male', 'military personnel', 'prospective studies', 'risk factors', 'teaching', 'thailand', 'wounds and injuries']\"\n",
      "\n",
      "Message saved to preprocess_data.csv\n",
      "Sending CSV content to Kafka: title,categories/keyword,author_tags\n",
      "incidence risk factor associated injury static line parachute t...\n"
     ]
    }
   ],
   "source": [
    "def consume_and_send():\n",
    "    output_file_path = 'preprocess_data.csv'\n",
    "\n",
    "    try:\n",
    "        print(\"Waiting for data\")\n",
    "        for message in consumer:\n",
    "            print(message )\n",
    "            print(f\"Received message: [{message.timestamp}:{message.offset}] {message.value}\")\n",
    "            \n",
    "            # Write the received message (CSV content) to a file\n",
    "            with open(output_file_path, 'w') as f:\n",
    "                f.write(message.value)  # Save the content to the file\n",
    "\n",
    "            print(f\"Message saved to {output_file_path}\")\n",
    "            \n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(output_file_path)\n",
    "            \n",
    "            # Data cleaning steps\n",
    "            df['title'] = df['title'].apply(preprocess_text)\n",
    "            df['categories/keyword'] = df['categories/keyword'].apply(preprocess_text)\n",
    "            df['author_tags'] = df['author_tags'].apply(preprocess_text)\n",
    "            # Save the cleaned DataFrame\n",
    "            \n",
    "            # Save cleaned data to a new CSV\n",
    "            df.to_csv(output_file_path , index=False)\n",
    "            with open(output_file_path, 'r') as csvfile:\n",
    "                csv_content = csvfile.read()  # Read entire file content\n",
    "            print(f\"Sending CSV content to Kafka: {csv_content[:100]}...\")  # Log first 100 characters\n",
    "            producer.send('for_ML_data', csv_content.encode('utf-8'))\n",
    "            time.sleep(2)\n",
    "            producer.flush()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while consuming: {e}\")\n",
    "\n",
    "# # Producer function\n",
    "# def produce_messages():\n",
    "#     csv_file_path = 'preprocess_data.csv'\n",
    "\n",
    "#     while not os.path.exists(csv_file_path):\n",
    "#         print(f\"Waiting for {csv_file_path} to appear...\")\n",
    "#         time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "\n",
    "#     print(f\"{csv_file_path} found! Proceeding to send data...\")\n",
    "#     with open(csv_file_path, 'r') as csvfile:\n",
    "#         csv_content = csvfile.read()  # Read entire file content\n",
    "\n",
    "#     # Send the entire CSV content to Kafka as a single message\n",
    "#     print(f'Sending entire CSV content to Kafka: {csv_content[:100]}...')  # Display first 100 chars for logging\n",
    "#     producer.send('preprocess_data', csv_content.encode('utf-8'))  # Send the CSV content\n",
    "#     time.sleep(2)\n",
    "\n",
    "\n",
    "#     # Ensure all messages are sent before exiting\n",
    "#     producer.flush()\n",
    "\n",
    "# # Running producer and consumer in separate threads\n",
    "# producer_thread = threading.Thread(target=produce_messages)\n",
    "# consumer_thread = threading.Thread(target=consume_messages)\n",
    "\n",
    "# producer_thread.start()\n",
    "# consumer_thread.start()\n",
    "\n",
    "# producer_thread.join()\n",
    "# consumer_thread.join()\n",
    "try:\n",
    "    consume_and_send()\n",
    "except Exception as e:\n",
    "    print(f\"Error while calling: {e}\")\n",
    "#         retry_count += 1  # Increment retry count for unexpected errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/2018_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_titles = TfidfVectorizer(max_features=10000, min_df=1, max_df=0.8)\n",
    "df['title_tfidf'] = list(tfidf_vectorizer_titles.fit_transform(df['title']).toarray())\n",
    "\n",
    "# Sentence Embeddings for abstracts\n",
    "# df['tags_tfidf'] = list(tfidf_vectorizer.fit_transform(df['author_tags']).toarray())\n",
    "\n",
    "# Save extracted features\n",
    "df.to_pickle(\"processed_features.pkl\")\n",
    "# df['author_tags'].head()\n",
    "# df[['title_tfidf', 'tags_tfidf']].head()\n",
    "# Get the feature names (words) from the vectorizer\n",
    "vocab = tfidf_vectorizer_titles.get_feature_names_out()\n",
    "title_tfidf_vector = df['title_tfidf'][1]  # TF-IDF vector\n",
    "nonzero_indices = [i for i, value in enumerate(title_tfidf_vector) if value != 0]\n",
    "\n",
    "# Print words and their corresponding TF-IDF scores\n",
    "for index in nonzero_indices:\n",
    "    print(f\"Word: {vocab[index]}, TF-IDF Score: {title_tfidf_vector[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to specific columns (e.g., 'title' and 'abstract')\n",
    "df['title'] = df['title'].apply(preprocess_text)\n",
    "df['categories/keyword'] = df['categories/keyword'].apply(preprocess_text)\n",
    "df['author_tags'] = df['author_tags'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "df.to_csv('data/2018_cleaned_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('data/2018_cleaned_processed.csv')\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pipeline for feature extraction\n",
    "# TF-IDF for titles\n",
    "tfidf_vectorizer_titles = TfidfVectorizer(max_features=10000, min_df=0.01, max_df=0.8) #exclude > 80% duplicate and < 1%\n",
    "df['title_tfidf']  = list(tfidf_vectorizer_titles.fit_transform(df['title']).toarray())\n",
    "\n",
    "X_tfidf = tfidf_vectorizer_titles.fit_transform(df['title']).toarray()\n",
    "\n",
    "# Get the feature names (the words)\n",
    "header = tfidf_vectorizer_titles.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF array to a DataFrame\n",
    "df_tfidf = pd.DataFrame(X_tfidf, columns=header)\n",
    "\n",
    "# Now you can add this DataFrame back to your original DataFrame\n",
    "df = pd.concat([df, df_tfidf], axis=1)\n",
    "df = df.drop(columns=['title_tfidf'])\n",
    "\n",
    "df.head()\n",
    "# sample_tfidf = tfidf_vectorizer_titles.fit_transform()\n",
    "\n",
    "\n",
    "# # Sentence Embeddings for abstracts\n",
    "# # df['tags_tfidf'] = list(tfidf_vectorizer.fit_transform(df['author_tags']).toarray())\n",
    "\n",
    "# # Save extracted features\n",
    "# df.to_pickle(\"processed_features.pkl\")\n",
    "# # df['author_tags'].head()\n",
    "# # df[['title_tfidf', 'tags_tfidf']].head()\n",
    "\n",
    "# print(name_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_titles = TfidfVectorizer(max_features=5000, min_df=0.01, max_df=0.8)\n",
    "title_tfidf = tfidf_vectorizer_titles.fit_transform(df['title'])\n",
    "\n",
    "# TF-IDF for tags\n",
    "tfidf_vectorizer_tags = TfidfVectorizer(max_features=5000, min_df=0.01, max_df=0.8)\n",
    "tags_tfidf = tfidf_vectorizer_tags.fit_transform(df['author_tags'])\n",
    "\n",
    "# Optionally, get the feature names (terms)\n",
    "title_feature_names = tfidf_vectorizer_titles.get_feature_names_out()\n",
    "tags_feature_names = tfidf_vectorizer_tags.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Combine the separate TF-IDF features into one feature matrix\n",
    "X = hstack([title_tfidf, tags_tfidf])\n",
    "\n",
    "# Convert the combined features to a dense format (optional, depending on model)\n",
    "# X = X.toarray()  # Only if your model requires dense arrays, some models like Logistic Regression do\n",
    "X.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
