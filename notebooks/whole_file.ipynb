{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to D:/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to D:/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to D:/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "nltk_data_path = \"D:/nltk_data\"  # Change this to your desired directory\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "# Append the path to NLTK's data search paths\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "# Download the required NLTK data to the custom path\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
    "nltk.data.path.append(\"D:/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker = 'localhost:29092'\n",
    "consumer = KafkaConsumer(\n",
    "    \"for_ML_data\",\n",
    "    bootstrap_servers=[kafka_broker],\n",
    "    enable_auto_commit=True,\n",
    "    value_deserializer=lambda x: x.decode('utf-8')\n",
    ")\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[kafka_broker],\n",
    "    linger_ms=5000,\n",
    "    acks='all',\n",
    "    max_block_ms=60000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove HTML tags, special characters, and punctuation.\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and punctuation\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # mapped_tokens = [synonym_mapping.get(token, token) for token in lemmatized_tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text, language='english', preserve_line=True)\n",
    "    return tokens\n",
    "\n",
    "def handle_missing_data(text, placeholder='Missing'):\n",
    "    \"\"\"Handle missing or noisy data.\"\"\"\n",
    "    if pd.isnull(text) or text.strip() == \"\":\n",
    "        return placeholder\n",
    "    return text\n",
    "\n",
    "def to_lowercase(tokens):\n",
    "    \"\"\"Convert tokens to lowercase.\"\"\"\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Complete text preprocessing pipeline.\"\"\"\n",
    "    text = handle_missing_data(text)  # Handle missing or noisy data\n",
    "    # print(f\"Handled Missing Data: {text}\")\n",
    "    text = clean_text(text)          # Clean text (remove unwanted characters)\n",
    "    # print(f\"Cleaned Text: {text}\")\n",
    "    tokens = tokenize_text(text)     # Tokenize text\n",
    "    # print(f\"Tokenized Text: {tokens}\")\n",
    "    tokens = to_lowercase(tokens)    # Convert to lowercase\n",
    "    # print(f\"Lowercase Tokens: {tokens}\")\n",
    "    tokens = remove_stopwords(tokens)  # Remove stopwords\n",
    "    # print(f\"Tokens after Removing Stopwords: {tokens}\")\n",
    "    tokens = lemmatize_tokens(tokens)  # Or stem_tokens(tokens) for stemming\n",
    "    # print(f\"Lemmatized and Mapped Tokens: {tokens}\")\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_cleaned_ml.csv')\n",
    "df['title'] = df['title'].apply(preprocess_text)\n",
    "df['categories/keyword'] = df['categories/keyword'].apply(preprocess_text)\n",
    "df['author_tags'] = df['author_tags'].apply(preprocess_text)\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "#df.to_csv('cleaned_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_words = {\"linking\",\"impact\",\"factor\",\"evaluation\",\"development\",\"associated\", \"approach\",\"activity\",\"new\", \"expert\",\n",
    "                    \"expression\", \"application\", \"case\",\"letter\", \"research\", \"paper\", \"journal\", \"today\", \"study\", \"review\",\"using\", \n",
    "                    \"property\",\"production\",\"update\", \"based\", \"change\", \"applied\", \"access\", \"report\", \"high\", \"plo\", \"zootaxa\", \"novel\", \"scopus\", \"outcome\", \"tev\", \"performance\"\n",
    "                    ,\"effect\", \"response\",\"journal\", \"effectiveness\", \"modified\", \"systematic\", \"use\", \"trial\", \"state\", \"acute\", \"analysis\", \"association\", \"characteristic\", \n",
    "                    \"comparison\", \"composite\", \"control\", \"controlled\", \"different\", \"process\", \"acm\", \"thailand\",\"proceeding\", \"iop\",\"frontier\", \"treatment\", \"model\", \"conference\", \"international\"\n",
    "                    ,\"series\", \"science\",\"engineering\", \"asian\", \"one\", \"scientific\", \"b\", \"among\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictedModel(input_text, save_df, cat_num, max_df, min_df):\n",
    "    \n",
    "    df = save_df\n",
    "    # Words to exclude\n",
    "    exclude_words = {\"linking\",\"impact\",\"factor\",\"evaluation\",\"development\",\"associated\", \"approach\",\"activity\",\"new\", \"expert\",\n",
    "                    \"expression\", \"application\", \"case\",\"letter\", \"research\", \"paper\", \"journal\", \"today\", \"study\", \"review\",\"using\", \n",
    "                    \"property\",\"production\",\"update\", \"based\", \"change\", \"applied\", \"access\", \"report\", \"high\", \"plo\", \"zootaxa\", \"novel\", \"scopus\", \"outcome\", \"tev\", \"performance\"\n",
    "                    ,\"effect\", \"response\",\"journal\", \"effectiveness\", \"modified\", \"systematic\", \"use\", \"trial\", \"state\", \"acute\", \"analysis\", \"association\", \"characteristic\", \n",
    "                    \"comparison\", \"composite\", \"control\", \"controlled\", \"different\", \"process\", \"acm\", \"thailand\",\"proceeding\", \"iop\",\"frontier\", \"treatment\", \"model\", \"conference\", \"international\"\n",
    "                    ,\"series\", \"science\",\"engineering\", \"asian\", \"one\", \"scientific\", \"b\", \"among\" }\n",
    "\n",
    "    def preprocess_text(text, exclude_words):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())  # Tokenize and convert to lowercase\n",
    "        return [word for word in words if word not in exclude_words]\n",
    "    # Process the categories column\n",
    "    all_words_y = []\n",
    "    for sentence in df['categories/keyword']:\n",
    "        all_words_y.extend(preprocess_text(sentence, exclude_words))\n",
    "\n",
    "    # Count the words\n",
    "    word_counts = Counter(all_words_y)\n",
    "    # Get the most common words\n",
    "    most_common_words_y = word_counts.most_common(cat_num)  # Top 10 words\n",
    "    # Extract words and counts for plotting\n",
    "    words_y, counts = zip(*most_common_words_y)\n",
    "\n",
    "    Y_data = list(words_y)\n",
    "    # Show the plot\n",
    "    def substitute_row(row, top_words):\n",
    "        words = preprocess_text(row, exclude_words)\n",
    "        for word in words:\n",
    "            if word in top_words:\n",
    "                return word  # Replace the row with the matched top word\n",
    "        return row  # Keep original if no top word matches\n",
    "\n",
    "    df['new_categories'] = df['categories/keyword'].apply(lambda x: substitute_row(x, Y_data))\n",
    "    def contains_top_word(row, top_words):\n",
    "    # Tokenize the row text and check for intersection with top words\n",
    "        words = set(preprocess_text(row, exclude_words))\n",
    "        return bool(words.intersection(top_words))\n",
    "\n",
    "    df_Y_data = df[df['categories/keyword'].apply(lambda x: contains_top_word(x, Y_data))].reset_index(drop=True)\n",
    "\n",
    "    # Get the indices of the rows that match the condition\n",
    "    matching_indices = df[df['categories/keyword'].apply(lambda x: contains_top_word(x, Y_data))].index\n",
    "\n",
    "    df_X_data =  df.loc[matching_indices].reset_index(drop=True)\n",
    "\n",
    "    # Check which rows are being selected (for debugging)\n",
    "    # print(\"Rows to be dropped (df_Y_data):\")\n",
    "    # print(len(matching_indices))\n",
    "\n",
    "    # Get the indices of the rows to drop from the original df\n",
    "    rows_to_drop = df_Y_data.index\n",
    "\n",
    "    # Drop the rows from the original df\n",
    "    df_dropped = df.drop(rows_to_drop).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Recount occurrences of Top 10 words in the filtered DataFrame\n",
    "    filtered_word_counts = Counter()\n",
    "    for row in df['categories/keyword']:\n",
    "        words = preprocess_text(row, exclude_words)\n",
    "        for word in words:\n",
    "            if word in Y_data:\n",
    "                filtered_word_counts[word] += 1\n",
    "\n",
    "    # Extract words and counts for plotting\n",
    "    words, counts = zip(*filtered_word_counts.items())\n",
    "\n",
    "    # Plotting\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # sns.barplot(x=list(counts), y=list(words), palette=\"viridis\")\n",
    "    # plt.title(\"Counts of Top 10 Words in Filtered DataFrame\", fontsize=16)\n",
    "    # plt.xlabel(\"Count\", fontsize=14)\n",
    "    # plt.ylabel(\"Words\", fontsize=14)\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    # Print the filtered DataFrame\n",
    "    # print(\"Filtered DataFrame (Only Rows with Top 10 Words):\")\n",
    "\n",
    "    # Show the plot\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "# Instantiate the CountVectorizer\n",
    "    for category in Y_data:\n",
    "        df_Y_data[category] = df_Y_data['new_categories'].apply(lambda x: 1 if category in x.split() else 0)\n",
    "\n",
    "    # Display the updated DataFrame\n",
    "    df_Y_data.drop(columns=[\"new_categories\", \"author_tags\",\"categories/keyword\"], inplace=True)\n",
    "\n",
    "    custom_stop_words = list(set(TfidfVectorizer(stop_words='english').get_stop_words()).union(exclude_words))\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, stop_words=custom_stop_words, max_df=max_df)\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df_X_data['title'])\n",
    "    columns2 = tfidf_vectorizer.get_feature_names_out()\n",
    "    X_df = pd.DataFrame(X_tfidf .toarray(), columns=columns2)\n",
    "\n",
    "\n",
    "    all_zero_rows = (X_df == 0).all(axis=1) \n",
    "# Count rows that have all zeros \n",
    "    count_all_zero_rows = all_zero_rows.sum() \n",
    "    # print(f\"Number of rows with all zeros: {count_all_zero_rows} from {X_df.shape[0]}\")\n",
    "\n",
    "    # List the indices of rows that have all elements equal to zero \n",
    "    zero_row_indices = all_zero_rows[all_zero_rows].index.tolist() \n",
    "    # print(f\"Indices of rows with all zeros: {zero_row_indices}\")\n",
    "    X_input = X_df.drop(zero_row_indices).reset_index(drop=True) \n",
    "    Y_output = df_Y_data.drop(zero_row_indices).reset_index(drop=True)\n",
    "    Y_output = Y_output.drop(columns=\"title\")\n",
    "\n",
    "    # Assuming X_input and Y_output are already defined\n",
    "    X = X_input \n",
    "    y = Y_output\n",
    "\n",
    "    # Step 3: Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Step 4: Train the Multi-Label Model using MultiOutputClassifier\n",
    "    mnb = MultinomialNB()\n",
    "    multi_target_mnb = MultiOutputClassifier(mnb, n_jobs=-1)\n",
    "    multi_target_mnb.fit(X_train, y_train)\n",
    "\n",
    "    # Step 5: Predict on test data\n",
    "    y_pred = multi_target_mnb.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, target_names=Y_data, output_dict=True)\n",
    "    report = classification_report(y_test, y_pred, target_names=Y_data)\n",
    "    print(report)\n",
    "\n",
    "    return multi_target_mnb, tfidf_vectorizer, Y_data\n",
    "  \n",
    "    # Step 6: Evaluate the model\n",
    "    # return classification_report(y_test, y_pred, target_names=Y_data)  # Ensure Y_data matches labels in y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    medicine       0.89      0.88      0.88       123\n",
      "    material       0.72      0.52      0.61       115\n",
      "      energy       0.77      0.60      0.68       120\n",
      "      physic       0.77      0.18      0.29        55\n",
      "  technology       0.90      0.23      0.37       111\n",
      "\n",
      "   micro avg       0.81      0.53      0.64       524\n",
      "   macro avg       0.81      0.48      0.57       524\n",
      "weighted avg       0.81      0.53      0.60       524\n",
      " samples avg       0.52      0.53      0.52       524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MultiOutputClassifier(estimator=MultinomialNB(), n_jobs=-1),\n",
       " TfidfVectorizer(max_df=0.99, min_df=0.001,\n",
       "                 stop_words=['do', 'were', 'that', 'approach', 'bill', 'letter',\n",
       "                             'his', 'about', 'another', 'together', 'several',\n",
       "                             'under', 'someone', 'which', 'amongst', 'below',\n",
       "                             'paper', 'composite', 'nobody', 'take', 'towards',\n",
       "                             'whoever', 'herein', 'have', 'de', 'study',\n",
       "                             'whether', 'no', 'anywhere', 'co', ...]),\n",
       " ['medicine', 'material', 'energy', 'physic', 'technology'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedModel(\"\", df, 5,0.99, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasker_df = pd.read_csv('lasker_cleaned_ml.csv')\n",
    "lasker_df['title'] = lasker_df['title'].apply(preprocess_text)\n",
    "#lasker_df['categories/keyword'] = df['categories/keyword'].apply(preprocess_text)\n",
    "\n",
    "nobel_df = pd.read_csv('nobelPrize_cleaned_ml.csv')\n",
    "nobel_df['title'] = nobel_df['title'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_new_data(trained_model, tfidf_vectorizer, Y_data, new_dataset, exclude_words):\n",
    "    \"\"\"\n",
    "    Predict labels for a new dataset with only a 'title' column.\n",
    "\n",
    "    Parameters:\n",
    "    - trained_model: The trained multi-label model (MultiOutputClassifier).\n",
    "    - tfidf_vectorizer: The fitted TfidfVectorizer.\n",
    "    - Y_data: List of target labels used in training.\n",
    "    - new_dataset: DataFrame containing the new data to predict. Must have a 'title' column.\n",
    "    - exclude_words: Set of words to exclude during preprocessing.\n",
    "\n",
    "    Returns:\n",
    "    - predictions_df: DataFrame containing the titles and their predicted labels.\n",
    "    \"\"\"\n",
    "    def preprocess_text(text, exclude_words):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())  # Tokenize and convert to lowercase\n",
    "        return [word for word in words if word not in exclude_words]\n",
    "\n",
    "    # Preprocess the 'title' column of the new dataset\n",
    "    new_dataset['processed_title'] = new_dataset['title'].apply(lambda x: \" \".join(preprocess_text(x, exclude_words)))\n",
    "\n",
    "    # Transform the new dataset using the fitted TfidfVectorizer\n",
    "    X_new_tfidf = tfidf_vectorizer.transform(new_dataset['processed_title'])\n",
    "\n",
    "    # Predict labels using the trained model\n",
    "    y_pred = trained_model.predict(X_new_tfidf)\n",
    "\n",
    "    # Convert predictions into a DataFrame\n",
    "    predictions_df = pd.DataFrame(y_pred, columns=Y_data)\n",
    "\n",
    "    # Add the titles back to the predictions DataFrame\n",
    "    predictions_df.insert(0, 'title', new_dataset['title'])\n",
    "\n",
    "    predictions_df.to_csv('prediction.csv', index=False)\n",
    "\n",
    "    return predictions_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    medicine       0.89      0.88      0.88       123\n",
      "    material       0.72      0.52      0.61       115\n",
      "      energy       0.77      0.60      0.68       120\n",
      "      physic       0.77      0.18      0.29        55\n",
      "  technology       0.90      0.23      0.37       111\n",
      "\n",
      "   micro avg       0.81      0.53      0.64       524\n",
      "   macro avg       0.81      0.48      0.57       524\n",
      "weighted avg       0.81      0.53      0.60       524\n",
      " samples avg       0.52      0.53      0.52       524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Asus\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "multi_target_mnb, tfidf_vectorizer, Y_data = predictedModel(\"\", df, 5,0.99, 0.001)\n",
    "\n",
    "predictions = predict_for_new_data(\n",
    "    trained_model=multi_target_mnb,\n",
    "    tfidf_vectorizer=tfidf_vectorizer,\n",
    "    Y_data=Y_data,\n",
    "    new_dataset=lasker_df,  # DataFrame with only the 'title' column\n",
    "    exclude_words=exclude_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_for_new_data(\n",
    "    trained_model=multi_target_mnb,\n",
    "    tfidf_vectorizer=tfidf_vectorizer,\n",
    "    Y_data=Y_data,\n",
    "    new_dataset=nobel_df,  # DataFrame with only the 'title' column\n",
    "    exclude_words=exclude_words\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
