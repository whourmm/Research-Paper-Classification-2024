{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening page: https://laskerfoundation.org/all-awards-winners/\n",
      "Finished scraping: https://laskerfoundation.org/all-awards-winners/\n",
      "Collected 20 items.\n",
      "Data saved to extracted_data.csv\n",
      "Total data collected: 20\n"
     ]
    }
   ],
   "source": [
    "def extract_data(url, target_data_count=20, output_csv='extracted_data.csv'):\n",
    "    print(f\"Opening page: {url}\")  # Print progress as soon as the page starts loading\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration (useful for some environments)\n",
    "    webdriver_path = r\"C:\\users\\asus\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "    # Create a Service object\n",
    "    service = Service(webdriver_path)\n",
    "    \n",
    "    # Initialize the WebDriver using the Service object\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "        data = []\n",
    "\n",
    "        # Scroll down to load more content\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        collected_count = 0  # Counter for the number of collected items\n",
    "\n",
    "        while collected_count < target_data_count:\n",
    "            # Find the articles on the page\n",
    "            articles = soup.find_all('article')\n",
    "            for article in articles:\n",
    "                title_author = article.find('div', {'class': 'fusion-post-content post-content'})\n",
    "                a_title = title_author.find('a')\n",
    "                author_name = title_author.find('p')\n",
    "                title_link = a_title['href'] if a_title else None \n",
    "\n",
    "\n",
    "                year_prize_a = article.find('div', {'class' : \"fusion-meta-info\"})\n",
    "                year_prize = year_prize_a.find_all('a')\n",
    "                year = year_prize[1]\n",
    "                prize = year_prize[2]\n",
    "\n",
    "\n",
    "\n",
    "                if a_title and author_name:\n",
    "                    data.append([a_title.text.strip(), author_name.text.strip(), year.text.strip(), prize.text.strip(), title_link])  # Append both title and author to the list\n",
    "                    collected_count += 1\n",
    "\n",
    "                # Stop if we have collected the target amount\n",
    "                if collected_count >= target_data_count:\n",
    "                    break\n",
    "\n",
    "            # If we've collected enough data, exit the loop\n",
    "            if collected_count >= target_data_count:\n",
    "                break\n",
    "\n",
    "            # Scroll down to the bottom of the page to load more content\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait for the new content to load (adjust timeout as needed)\n",
    "            time.sleep(3)  # Wait for 3 seconds for the page to load more data\n",
    "\n",
    "            # Check if the page height is increased (indicating more content was loaded)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break  # No more new content, exit the loop\n",
    "            last_height = new_height  # Update the last height for the next iteration\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")  # Log errors during scraping\n",
    "        data = []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    print(f\"Finished scraping: {url}\")  # Notify when scraping for this URL is complete\n",
    "    print(f\"Collected {collected_count} items.\")\n",
    "    \n",
    "    # Save data to CSV\n",
    "    save_to_csv(data, output_csv)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Saves the collected data to a CSV file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Title', 'Author', 'Year', 'Type of Prize', 'url'])  # Write the header row with both Title and Author\n",
    "            writer.writerows(data)  # Write the collected data\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to CSV: {e}\")\n",
    "\n",
    "# Example usage\n",
    "extracted_data = extract_data(\"https://laskerfoundation.org/all-awards-winners/\")\n",
    "print(f\"Total data collected: {len(extracted_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import required libraries\n",
    "# from kafka import KafkaProducer\n",
    "# import time\n",
    "# from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Connect to kafka broker running in your local host (docker). Change this to your kafka broker if needed\n",
    "# kafka_broker = 'localhost:29092'\n",
    "# producer = KafkaProducer(\n",
    "#     bootstrap_servers=[kafka_broker],\n",
    "#     linger_ms=5000,  # Increased linger time\n",
    "#     max_block_ms=60000,  # Increase the max block time (default: 60000 ms = 1 minute)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
