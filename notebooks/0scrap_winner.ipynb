{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening page: https://www.nobelprize.org/prizes/lists/all-nobel-prizes/\n",
      "Clicked cookie banner accept button.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 90 new articles.\n",
      "Found 90 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 90 new articles.\n",
      "Found 90 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 150 new articles.\n",
      "Found 150 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 270 new articles.\n",
      "Found 270 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 330 new articles.\n",
      "Found 330 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 381 new articles.\n",
      "Found 381 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 431 new articles.\n",
      "Found 431 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 481 new articles.\n",
      "Found 481 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 531 new articles.\n",
      "Found 531 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 581 new articles.\n",
      "Found 581 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 631 new articles.\n",
      "Found 631 articles so far.\n",
      "Clicked 'Load More' button successfully.\n",
      "Found 676 new articles.\n",
      "Found 676 articles so far.\n",
      "Load More button not found or error: Message: \n",
      "\n",
      "Finished scraping: https://www.nobelprize.org/prizes/lists/all-nobel-prizes/\n",
      "Data saved to extracted_data_2.csv\n",
      "Total data collected: 4448\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def extract_data(url, target_data_count=800, output_csv='extracted_data_2.csv'):\n",
    "    print(f\"Opening page: {url}\")  # Print progress as soon as the page starts loading\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration (useful for some environments)\n",
    "    webdriver_path = r\"C:\\Users\\admin\\Downloads\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "    # Create a Service object\n",
    "    service = Service(webdriver_path)\n",
    "    \n",
    "    # Initialize the WebDriver using the Service object\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")  # Python HTML parser\n",
    "        data = []\n",
    "\n",
    "        articles = soup.find_all('div', {'class': 'card-prize'})\n",
    "        cookie_found = False\n",
    "\n",
    "        while(len(articles) < 800): \n",
    "            \n",
    "            if(not cookie_found):\n",
    "                time.sleep(2)\n",
    "                cookie_banner = WebDriverWait(driver, 10).until(\n",
    "                    EC.visibility_of_element_located((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "                )\n",
    "                if(cookie_banner) : \n",
    "                    cookie_banner.click()  # Click the \"Accept\" button or close the banner\n",
    "                    print(\"Clicked cookie banner accept button.\")\n",
    "                    cookie_found = True\n",
    "\n",
    "            # Wait for and click the \"Load More\" button\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CLASS_NAME, \"dynamic-list-load-more\"))\n",
    "                )\n",
    "                load_more_button.click()  # Click the 'Load More' button\n",
    "                print(\"Clicked 'Load More' button successfully.\")\n",
    "            except Exception as e:\n",
    "                print(\"Load More button not found or error:\", e)\n",
    "                break  # If no \"Load More\" button is found, exit the loop\n",
    "\n",
    "            # Wait for the page to load more content\n",
    "            time.sleep(3)  # Wait for 3 seconds for the page to load more data\n",
    "\n",
    "            # Re-parse the page again after clicking 'Load More'\n",
    "            soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "            # Extract articles\n",
    "            articles = soup.find_all('div', {'class': 'card-prize'})\n",
    "            print(f\"Found {len(articles)} new articles.\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            articles = soup.find_all('div', {'class': 'card-prize'})\n",
    "            print(f\"Found {len(articles)} articles so far.\")\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    author_section = article.find('div', {'class': 'card-prize--laureates--links'})\n",
    "                    authors = [author.text.strip() for author in author_section.find_all('a')] if author_section else []\n",
    "\n",
    "                    title = article.find('blockquote', {'class': 'card-prize--laureates--motivation --last'})\n",
    "                    title_text = title.text.strip() if title else \"\"\n",
    "                    clean_title = title_text.replace(\"“\", \"\").replace(\"”\", \"\").replace('\"', '').lstrip(\"for \").strip()\n",
    "\n",
    "                    element = article.find('h3')\n",
    "                    element_a = element.find('a') if element else None\n",
    "                    link = element_a['href'] if element_a else None\n",
    "                    full_text = element_a.text if element_a else \"\"\n",
    "\n",
    "                    parts = full_text.split(\" \")\n",
    "                    if len(parts) >= 3:\n",
    "                        _, subject, year = parts[-3:]\n",
    "                    else:\n",
    "                        subject, year = \"Unknown\", \"Unknown\"\n",
    "\n",
    "                    if clean_title and authors:\n",
    "                        data.append([clean_title, authors, year, subject + ' Nobel prize', link])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article: {e}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")  # Log errors during scraping\n",
    "        data = []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    print(f\"Finished scraping: {url}\")  # Notify when scraping for this URL is complete\n",
    "    # Save data to CSV\n",
    "    save_to_csv(data, output_csv)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Saves the collected data to a CSV file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Title', 'Author', 'Year', 'Type of Prize', 'url'])  # Write the header row with both Title and Author\n",
    "            writer.writerows(data)  # Write the collected data\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to CSV: {e}\")\n",
    "extracted_data = extract_data(\"https://www.nobelprize.org/prizes/lists/all-nobel-prizes/\")\n",
    "print(f\"Total data collected: {len(extracted_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import required libraries\n",
    "# from kafka import KafkaProducer\n",
    "# import time\n",
    "# from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Connect to kafka broker running in your local host (docker). Change this to your kafka broker if needed\n",
    "# kafka_broker = 'localhost:29092'\n",
    "# producer = KafkaProducer(\n",
    "#     bootstrap_servers=[kafka_broker],\n",
    "#     linger_ms=5000,  # Increased linger time\n",
    "#     max_block_ms=60000,  # Increase the max block time (default: 60000 ms = 1 minute)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSDE-CEDT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
