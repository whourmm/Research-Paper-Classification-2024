{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **| Scrapping 1,000 Additional Data for predicting which categories have the most oppotunity to get the research prize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re\n",
    "import time\n",
    "# import required libraries\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **| Connect to Kafka** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to kafka broker running in your local host (docker). Change this to your kafka broker if needed\n",
    "kafka_broker = 'localhost:29092'\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[kafka_broker],\n",
    "    linger_ms=5000,  # Increased linger time\n",
    "    max_block_ms=60000,  # Increase the max block time (default: 60000 ms = 1 minute)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **| Extract Data** \n",
    "##### 1. extract_data_lasker(url, target_data_count=20, output_csv='extracted_data.csv') \n",
    "This part will scrap data from https://laskerfoundation.org/all-awards-winners/ and get all winner detail in each link ~ 337 link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError saving data to CSV: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m extracted_data \u001b[38;5;241m=\u001b[39m extract_data_lasker(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://laskerfoundation.org/all-awards-winners/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal data collected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(extracted_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36mextract_data_lasker\u001b[1;34m(url, target_data_count, output_csv)\u001b[0m\n\u001b[0;32m     12\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mservice, options\u001b[38;5;241m=\u001b[39mchrome_options)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     16\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m     collected_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Counter for the number of collected items\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:393\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:382\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    380\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 382\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    136\u001b[0m         method,\n\u001b[0;32m    137\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m    144\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m    145\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\urllib3\\_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\urllib3\\poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\http\\client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1395\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\dsde-cedt\\Lib\\socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_data_lasker(url, target_data_count=20, output_csv='extracted_data.csv'):\n",
    "    print(f\"Opening page: {url}\")  # Print progress as soon as the page starts loading\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration (useful for some environments)\n",
    "    webdriver_path = r\"C:\\users\\asus\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "    # Create a Service object\n",
    "    service = Service(webdriver_path)\n",
    "    \n",
    "    # Initialize the WebDriver using the Service object\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        data = []\n",
    "        collected_count = 0  # Counter for the number of collected items\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "        articles = soup.find_all('article')\n",
    "        while(len(articles) < 500):\n",
    "            soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(3)\n",
    "            articles = soup.find_all('article')\n",
    "            print(len(articles))\n",
    "            if(len(articles) == 336):break\n",
    "        for article in articles:\n",
    "            title_author = article.find('div', {'class': 'fusion-post-content post-content'})\n",
    "            a_title = title_author.find('a')\n",
    "            author_name = title_author.find('p')\n",
    "            title_link = a_title['href'] if a_title else None \n",
    "\n",
    "\n",
    "            year_prize_a = article.find('div', {'class' : \"fusion-meta-info\"})\n",
    "            year_prize = year_prize_a.find_all('a')\n",
    "            year = year_prize[1]\n",
    "            prize = year_prize[2]\n",
    "\n",
    "\n",
    "\n",
    "            if a_title and author_name:\n",
    "                data.append([a_title.text.strip(), author_name.text.strip(), year.text.strip(), prize.text.strip(), title_link])  # Append both title and author to the list\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")  # Log errors during scraping\n",
    "        data = []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    print(f\"Finished scraping: {url}\")  # Notify when scraping for this URL is complete\n",
    "    print(f\"Collected {collected_count} items.\")\n",
    "    \n",
    "#     # Save data to CSV\n",
    "    save_to_csv(data, output_csv)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Saves the collected data to a CSV file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Title', 'Author', 'Year', 'Type of Prize', 'url'])  # Write the header row with both Title and Author\n",
    "            writer.writerows(data)  # Write the collected data\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to CSV: {e}\")\n",
    "\n",
    "# Example usage\n",
    "extracted_data = extract_data_lasker(\"https://laskerfoundation.org/all-awards-winners/\")\n",
    "print(f\"Total data collected: {len(extracted_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. extract_data(url, target_data_count=20, output_csv='extracted_data_2.csv') \n",
    "This part will scrap data from https://www.nobelprize.org/prizes/lists/all-nobel-prizes/ and get all winner detail in each link ~ 800 link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def extract_data(url, output_csv='extracted_data_2.csv'):\n",
    "    print(f\"Opening page: {url}\")  # Print progress as soon as the page starts loading\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration (useful for some environments)\n",
    "    webdriver_path = r\"C:\\users\\asus\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "    # Create a Service object\n",
    "    service = Service(webdriver_path)\n",
    "    \n",
    "    # Initialize the WebDriver using the Service object\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")  # Python HTML parser\n",
    "        data = []\n",
    "\n",
    "        articles = soup.find_all('div', {'class': 'card-prize'})\n",
    "        cookie_found = False\n",
    "\n",
    "        while(len(articles) < 40): \n",
    "            \n",
    "            if(not cookie_found):\n",
    "                time.sleep(2)\n",
    "                cookie_banner = WebDriverWait(driver, 10).until(\n",
    "                    EC.visibility_of_element_located((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "                )\n",
    "                if(cookie_banner) : \n",
    "                    cookie_banner.click()  # Click the \"Accept\" button or close the banner\n",
    "                    print(\"Clicked cookie banner accept button.\")\n",
    "                    cookie_found = True\n",
    "\n",
    "            # Wait for and click the \"Load More\" button\n",
    "            try:\n",
    "                load_more_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.CLASS_NAME, \"dynamic-list-load-more\"))\n",
    "                )\n",
    "                load_more_button.click()  # Click the 'Load More' button\n",
    "                print(\"Clicked 'Load More' button successfully.\")\n",
    "            except Exception as e:\n",
    "                print(\"Load More button not found or error:\", e)\n",
    "                break  # If no \"Load More\" button is found, exit the loop\n",
    "\n",
    "            # Wait for the page to load more content\n",
    "            time.sleep(3)  # Wait for 3 seconds for the page to load more data\n",
    "\n",
    "            # Re-parse the page again after clicking 'Load More'\n",
    "            soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "            # Extract articles\n",
    "            articles = soup.find_all('div', {'class': 'card-prize'})\n",
    "            print(f\"Found {len(articles)} new articles.\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            articles = soup.find_all('div', {'class': 'card-prize'})\n",
    "            print(f\"Found {len(articles)} articles so far.\")\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    author_section = article.find('div', {'class': 'card-prize--laureates--links'})\n",
    "                    authors = [author.text.strip() for author in author_section.find_all('a')] if author_section else []\n",
    "\n",
    "                    title = article.find('blockquote', {'class': 'card-prize--laureates--motivation --last'})\n",
    "                    title_text = title.text.strip() if title else \"\"\n",
    "                    clean_title = title_text.replace(\"“\", \"\").replace(\"”\", \"\").replace('\"', '').lstrip(\"for \").strip()\n",
    "\n",
    "                    element = article.find('h3')\n",
    "                    element_a = element.find('a') if element else None\n",
    "                    link = element_a['href'] if element_a else None\n",
    "                    full_text = element_a.text if element_a else \"\"\n",
    "\n",
    "                    parts = full_text.split(\" \")\n",
    "                    if len(parts) >= 3:\n",
    "                        _, subject, year = parts[-3:]\n",
    "                    else:\n",
    "                        subject, year = \"Unknown\", \"Unknown\"\n",
    "\n",
    "                    if clean_title and authors:\n",
    "                        data.append([clean_title, authors, year, subject + ' Nobel prize', link])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article: {e}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")  # Log errors during scraping\n",
    "        data = []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    print(f\"Finished scraping: {url}\")  # Notify when scraping for this URL is complete\n",
    "    # Save data to CSV\n",
    "    save_to_csv(data, output_csv)\n",
    "    with open(output_csv, 'r') as csvfile:\n",
    "        csv_content = csvfile.read()  # Read entire file content\n",
    "\n",
    "    # Send the entire CSV content to Kafka as a single message\n",
    "    print(f'Sending entire CSV content to Kafka: {csv_content[:100]}...')  # Display first 100 chars for logging\n",
    "    producer.send('data_extracted_nobel', csv_content.encode('utf-8'))  # Send the CSV content\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Ensure all messages are sent before exiting\n",
    "    producer.flush()\n",
    "    # if(i ==0): break\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Saves the collected data to a CSV file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Title', 'Author', 'Year', 'Type of Prize', 'url'])  # Write the header row with both Title and Author\n",
    "            writer.writerows(data)  # Write the collected data\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to CSV: {e}\")\n",
    "extracted_data = extract_data(\"https://www.nobelprize.org/prizes/lists/all-nobel-prizes/\")\n",
    "print(f\"Total data collected: {len(extracted_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
