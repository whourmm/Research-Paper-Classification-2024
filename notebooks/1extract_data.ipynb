{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Don't need to run this command I've already done for you) Extracted Link From the Given Json\n",
    "This part is for extracting the specific link that contain \"scopus.com\" but exclude \"citedby\" \n",
    "\n",
    "Example Link with <a href=\"https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85170238281&origin=inward\">\"citedby\"</a> : <br>\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../image/with_citeby.png\" alt=\"image\" width=\"1000\" height=\"500\">\n",
    "</div>\n",
    "</br>\n",
    "Example Link without <a href=\"https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050336797&origin=inward\">\"citedby\"</a> : <br>\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../image/without_citeby.png\" alt=\"image\" width=\"1000\" height=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scopus links have been saved in batches with a maximum of 200 links per file.\n"
     ]
    }
   ],
   "source": [
    "def extract_scopus_links(data, links=None):\n",
    "    \"\"\"\n",
    "    Recursively extract all Scopus links from a JSON object, excluding those with 'citedbyresults'.\n",
    "    \"\"\"\n",
    "    if links is None:\n",
    "        links = set()\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            extract_scopus_links(value, links)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            extract_scopus_links(item, links)\n",
    "    elif isinstance(data, str) and \"scopus.com\" in data and \"citedby\" not in data:\n",
    "        links.add(data)\n",
    "\n",
    "    return links\n",
    "\n",
    "def save_links_to_files(links, output_base, max_links_per_file):\n",
    "    \"\"\"\n",
    "    Save links to multiple files, each containing up to `max_links_per_file` links.\n",
    "    \"\"\"\n",
    "    links = list(links)\n",
    "    total_links = len(links)\n",
    "    num_files = (total_links // max_links_per_file) + (1 if total_links % max_links_per_file != 0 else 0)\n",
    "\n",
    "    for i in range(num_files):\n",
    "        start_idx = i * max_links_per_file\n",
    "        end_idx = start_idx + max_links_per_file\n",
    "        chunk_links = links[start_idx:end_idx]\n",
    "\n",
    "        output_file = f\"{output_base}_{i + 1}.txt\"\n",
    "        # Use utf-8 encoding when writing to the file\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(\"\\n\".join(chunk_links))\n",
    "        print(f\"Saved {len(chunk_links)} links to {output_file}.\")\n",
    "\n",
    "def process_folders(base_folder, output_base, max_links_per_file=200):\n",
    "    \"\"\"\n",
    "    Process all files in subfolders, extracting Scopus links from files containing JSON content.\n",
    "    \"\"\"\n",
    "    collected_links = set()\n",
    "\n",
    "    # Walk through each folder and file\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(f\"Processing folder: {root} | File: {file}\")  # Print current folder and file being processed\n",
    "            try:\n",
    "                # Try to open and parse the file as JSON, even if it's not a .json file\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        # Try loading the content as JSON\n",
    "                        json_data = json.load(f)\n",
    "\n",
    "                        # Extract Scopus links\n",
    "                        links = extract_scopus_links(json_data)\n",
    "                        collected_links.update(links)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # If the file content is not valid JSON, print a message\n",
    "                        print(f\"Skipping file (not valid JSON): {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    # Save collected links to multiple files\n",
    "    save_links_to_files(collected_links, output_base, max_links_per_file)\n",
    "\n",
    "# Step 1: Specify the folder where the extracted files are located\n",
    "base_folder = '../Project'  # Replace with the path to your extracted files\n",
    "\n",
    "# Step 2: Define the output base name for the link files\n",
    "output_base = 'scopus_links_2023'  # Base name for output files\n",
    "\n",
    "# Step 3: Process the folder and extract Scopus links, splitting into files with 200 links each\n",
    "process_folders(base_folder, output_base, max_links_per_file=200)\n",
    "\n",
    "# Step 4: Notify that the files are saved\n",
    "print(f\"Scopus links have been saved in batches with a maximum of 200 links per file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WebScrapping from the extracted link\n",
    "This part is for scraaping data including: \n",
    "- title\n",
    "- authors\n",
    "- article_info\n",
    "- abstract\n",
    "- categories/keyword\n",
    "- citation_info\n",
    "- document_info\n",
    "- author_tags\n",
    "- affiliations\n",
    "- funding\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../image/label_1.png\" alt=\"image\" width=\"1000\" height=\"500\">\n",
    "</div>\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../image/label_2.png\" alt=\"image\" width=\"1000\" height=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(url):\n",
    "    print(f\"Opening page: {url}\")  # Print progress as soon as the page starts loading\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration (useful for some environments)\n",
    "    webdriver_path = r\"C:\\users\\asus\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "# Create a Service object\n",
    "    service = Service(webdriver_path)\n",
    "\n",
    "# Optional: Add Chrome options if needed\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "# Initialize the WebDriver using the Service object\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "\n",
    "    try: \n",
    "\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "        data = {}\n",
    "\n",
    "        # Extraction logic (same as your provided code)\n",
    "        h2_elements = soup.select('h2')\n",
    "        for h2 in h2_elements:\n",
    "            inner_text = h2.get_text(strip=True)\n",
    "            data[\"title\"] = inner_text if inner_text else \"\"\n",
    "            # print(data)\n",
    "\n",
    "        author_section = soup.find('section', {'id': 'authorlist'})\n",
    "        if author_section:\n",
    "            author_tags = author_section.find_all('span', {'class': 'previewTxt'})\n",
    "            authors = [author_tag.get_text(strip=True) for author_tag in author_tags if author_tag.get_text(strip=True)]\n",
    "            data[\"authors\"] = authors if authors else []\n",
    "        else:\n",
    "            data[\"authors\"] = []\n",
    "\n",
    "        journal_info_span = soup.find('span', {'id': 'journalInfo'})\n",
    "        data[\"article_info\"] = journal_info_span.get_text(strip=True) if journal_info_span else \"\"\n",
    "\n",
    "        abstract_section = soup.find('section', {'id': 'abstractSection'})\n",
    "        if abstract_section:\n",
    "            p_tag = abstract_section.find('p')\n",
    "            data[\"abstract\"] = p_tag.get_text(strip=True) if p_tag else \"\"\n",
    "        else:\n",
    "            data[\"abstract\"] = \"\"\n",
    "\n",
    "        span_tag = soup.find('span', {'id': 'guestAccessSourceTitle'})\n",
    "        data[\"categories/keyword\"] = span_tag.get_text(strip=True) if span_tag else \"\"\n",
    "\n",
    "        citation_ul = soup.find('ul', {'id': 'citationInfo'})\n",
    "        citation_info = {}\n",
    "        citation_fields = [\"ISSN\", \"Source Type\", \"Original Language\"]\n",
    "        if citation_ul:\n",
    "            li_tags = citation_ul.find_all('li')\n",
    "            for i, li in enumerate(li_tags):\n",
    "                strong_tag = li.find('strong')\n",
    "                if strong_tag:\n",
    "                    strong_tag.extract()\n",
    "                clean_text = li.get_text(strip=True)\n",
    "                if i < len(citation_fields):\n",
    "                    citation_info[citation_fields[i]] = clean_text\n",
    "        data[\"citation_info\"] = citation_info\n",
    "\n",
    "        document_ul = soup.find('ul', {'id': 'documentInfo'})\n",
    "        document_info = {}\n",
    "        document_fields = [\"Document Type\", \"Publisher\"]\n",
    "        if document_ul:\n",
    "            li_tags = document_ul.find_all('li')\n",
    "            for i, li in enumerate(li_tags):\n",
    "                strong_tag = li.find('strong')\n",
    "                if strong_tag:\n",
    "                    strong_tag.extract()\n",
    "                clean_text = li.get_text(strip=True)\n",
    "                if i < len(document_fields):\n",
    "                    document_info[document_fields[i]] = clean_text\n",
    "        data[\"document_info\"] = document_info if document_info else {}\n",
    "\n",
    "        author_tags = soup.find_all('span', {'class': 'badges'})\n",
    "        authors = [author_tag.get_text(strip=True) for author_tag in author_tags if author_tag.get_text(strip=True)]\n",
    "        data[\"author_tags\"] = authors if authors else []\n",
    "\n",
    "        affiliation_section = soup.find('section', {'id': 'affiliationlist'})\n",
    "        if affiliation_section:\n",
    "            affiliation_tags = affiliation_section.find_all('li')\n",
    "            affiliations = [affiliation_tag.get_text(strip=True) for affiliation_tag in affiliation_tags if affiliation_tag.get_text(strip=True)]\n",
    "            data[\"affiliations\"] = affiliations if affiliations else []\n",
    "        else:\n",
    "            data[\"affiliations\"] = []\n",
    "\n",
    "        funding_rows = soup.find_all('tr', {'class': 'lightGreyBorderBottom'})\n",
    "        funding_data = []\n",
    "        for funding_row in funding_rows:\n",
    "            td_tags = funding_row.find_all('td')\n",
    "            funding_info = {\"Funding Sponsor\": \"\", \"Funding Number\": \"\", \"Acronym\": \"\"}\n",
    "            if len(td_tags) >= 3:\n",
    "                funding_info[\"Funding Sponsor\"] = td_tags[0].get_text(strip=True) if td_tags[0] else \"\"\n",
    "                funding_info[\"Funding Number\"] = td_tags[1].get_text(strip=True) if td_tags[1] else \"\"\n",
    "                funding_info[\"Acronym\"] = td_tags[2].get_text(strip=True) if td_tags[2] else \"\"\n",
    "            funding_data.append(funding_info)\n",
    "        data[\"funding\"] = funding_data if funding_data else []\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")  # Log errors during scraping\n",
    "        data = {}\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    print(f\"Finished scraping: {url}\")  # Notify when scraping for this URL is complete\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from kafka import KafkaProducer\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to kafka broker running in your local host (docker). Change this to your kafka broker if needed\n",
    "kafka_broker = 'localhost:29092'\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[kafka_broker],\n",
    "    linger_ms=5000,  # Increased linger time\n",
    "    max_block_ms=60000,  # Increase the max block time (default: 60000 ms = 1 minute)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data and send the data through Kafka Consumer\n",
    "1. Change ADD \"ADD PATH/TO/.TXT\" to the selected file path\n",
    "2. Change the output file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: scopus_links_2020_16.txt with 200 URLs.\n",
      "\n",
      "Processing URL 1/200...\n",
      "Opening page: https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086297374&origin=inward\n",
      "Finished scraping: https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85086297374&origin=inward\n",
      "Completed URL 1/200.\n",
      "Attempting to save to: ../extracted_data/extracted_2020\\scopus_links_2020_16_output.csv\n",
      "File saved: True\n",
      "Sending entire CSV content to Kafka: title,authors,article_info,abstract,categories/keyword,citation_info,document_info,author_tags,affil...\n",
      "All files processed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Folder containing the .txt files\n",
    "import os\n",
    "import pandas as pd\n",
    "folder_path = '../extracted_data/extracted_2020'  # Change to your folder path\n",
    "\n",
    "# List all .txt files in the folder\n",
    "txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "# Iterate through each .txt file\n",
    "i = 0\n",
    "for txt_file in txt_files:\n",
    "    file_path = os.path.join(folder_path, txt_file)\n",
    "\n",
    "    # Open and read URLs from the current file\n",
    "    with open(file_path, 'r') as file:\n",
    "        urls = [line.strip() for line in file if line.strip()]\n",
    "        # print(urls)\n",
    "\n",
    "    total_urls = len(urls)\n",
    "    print(f\"Processing file: {txt_file} with {total_urls} URLs.\")\n",
    "\n",
    "    all_data = []  # Collect all data for this file\n",
    "\n",
    "    # Extract data from the URLs in the current file\n",
    "    for index, url in enumerate(urls[:1], start=1):\n",
    "        print(f\"\\nProcessing URL {index}/{total_urls}...\")\n",
    "        # print(url)\n",
    "        try:\n",
    "            # Extract data for the current URL\n",
    "            extracted_data = extract_data(url)  # Calls the function that shows progress\n",
    "            all_data.append(extracted_data)\n",
    "            # print(all_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL {url}: {e}\")\n",
    "        print(f\"Completed URL {index}/{total_urls}.\")\n",
    "\n",
    "    # Convert extracted data into a pandas DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    # Define the output file name based on the .txt file\n",
    "    output_file = os.path.join(folder_path, f\"{os.path.splitext(txt_file)[0]}_output.csv\")\n",
    "    print(f\"Attempting to save to: {output_file}\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"File saved: {os.path.isfile(output_file)}\")\n",
    "\n",
    "    with open(output_file, 'r') as csvfile:\n",
    "        csv_content = csvfile.read()  # Read entire file content\n",
    "\n",
    "    # Send the entire CSV content to Kafka as a single message\n",
    "    print(f'Sending entire CSV content to Kafka: {csv_content[:100]}...')  # Display first 100 chars for logging\n",
    "    producer.send('data_extracted', csv_content.encode('utf-8'))  # Send the CSV content\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Ensure all messages are sent before exiting\n",
    "    producer.flush()\n",
    "    if(i ==0): break\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"All files processed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
