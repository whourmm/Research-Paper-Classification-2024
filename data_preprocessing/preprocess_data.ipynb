{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to D:/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to D:/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to D:/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk_data_path = \"D:/nltk_data\"  # Change this to your desired directory\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)\n",
    "\n",
    "# Append the path to NLTK's data search paths\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Download the required NLTK data to the custom path\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"D:/nltk_data\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text, language='english', preserve_line=True)\n",
    "    return tokens  # Output: ['This', 'is', 'a', 'sample', 'text', 'for', 'tokenization', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Step 1: Remove unwanted characters\n",
    "    text = clean_text(text)\n",
    "    # Step 2: Tokenize\n",
    "    tokens = tokenize_text(text)\n",
    "    # Step 3: Remove stop words\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'text', 'HTML', 'tags', 'punctuation', 'And', 'special', 'characters']\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"<p>This is an example text with HTML tags, punctuation! And special #characters?</p>\"\n",
    "processed_tokens = preprocess_text(raw_text)\n",
    "print(processed_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Remove HTML tags, special characters, and punctuation.\"\"\"\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and punctuation\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatize tokens.\"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize the text.\"\"\"\n",
    "    tokens = word_tokenize(text, language='english', preserve_line=True)\n",
    "    return tokens\n",
    "def handle_missing_data(text, placeholder='Missing'):\n",
    "    \"\"\"Handle missing or noisy data.\"\"\"\n",
    "    if pd.isnull(text) or text.strip() == \"\":\n",
    "        return placeholder\n",
    "    return text\n",
    "\n",
    "def to_lowercase(tokens):\n",
    "    \"\"\"Convert tokens to lowercase.\"\"\"\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Complete text preprocessing pipeline.\"\"\"\n",
    "    text = handle_missing_data(text)  # Handle missing or noisy data\n",
    "    text = clean_text(text)          # Clean text (remove unwanted characters)\n",
    "    tokens = tokenize_text(text)     # Tokenize text\n",
    "    tokens = to_lowercase(tokens)    # Convert to lowercase\n",
    "    tokens = remove_stopwords(tokens)  # Remove stopwords\n",
    "    tokens = lemmatize_tokens(tokens)  # Or stem_tokens(tokens) for stemming\n",
    "    return ' '.join(tokens)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>categories/keyword</th>\n",
       "      <th>author_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hydrogeological parameter distribution estimat...</td>\n",
       "      <td>international journal of civil engineering and...</td>\n",
       "      <td>['groundwater model', 'hydraulic conductivity'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>applying psychic distance to services internat...</td>\n",
       "      <td>journal of asia-pacific business</td>\n",
       "      <td>['cultural distance', 'health care services', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>estimation of aloe-emodin content in cassia gr...</td>\n",
       "      <td>indian journal of pharmaceutical sciences</td>\n",
       "      <td>['aloe-emodin contents', 'cassia garrettiana',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anti-rice pathogenic microbial activity of per...</td>\n",
       "      <td>science and technology asia</td>\n",
       "      <td>['antimicrobial activity', 'essential oil', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line-1 orf1 protein is up-regulated by reactiv...</td>\n",
       "      <td>cancer genomics and proteomics</td>\n",
       "      <td>['4-hne', 'bladder cancer', 'cancer progressio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  hydrogeological parameter distribution estimat...   \n",
       "1  applying psychic distance to services internat...   \n",
       "2  estimation of aloe-emodin content in cassia gr...   \n",
       "3  anti-rice pathogenic microbial activity of per...   \n",
       "4  line-1 orf1 protein is up-regulated by reactiv...   \n",
       "\n",
       "                                  categories/keyword  \\\n",
       "0  international journal of civil engineering and...   \n",
       "1                   journal of asia-pacific business   \n",
       "2          indian journal of pharmaceutical sciences   \n",
       "3                        science and technology asia   \n",
       "4                     cancer genomics and proteomics   \n",
       "\n",
       "                                         author_tags  \n",
       "0  ['groundwater model', 'hydraulic conductivity'...  \n",
       "1  ['cultural distance', 'health care services', ...  \n",
       "2  ['aloe-emodin contents', 'cassia garrettiana',...  \n",
       "3  ['antimicrobial activity', 'essential oil', 'p...  \n",
       "4  ['4-hne', 'bladder cancer', 'cancer progressio...  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/2018_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: applying, TF-IDF Score: 0.3403172942934855\n",
      "Word: caregiver, TF-IDF Score: 0.3179765854455264\n",
      "Word: case, TF-IDF Score: 0.21998024295224758\n",
      "Word: distance, TF-IDF Score: 0.3179765854455264\n",
      "Word: elderlyarticle, TF-IDF Score: 0.35805011251327074\n",
      "Word: internationalization, TF-IDF Score: 0.35805011251327074\n",
      "Word: japanese, TF-IDF Score: 0.327735659539065\n",
      "Word: psychic, TF-IDF Score: 0.35805011251327074\n",
      "Word: service, TF-IDF Score: 0.27294668374665065\n",
      "Word: study, TF-IDF Score: 0.18609364123438332\n",
      "Word: thai, TF-IDF Score: 0.1800932172800236\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer_titles = TfidfVectorizer(max_features=10000, min_df=1, max_df=0.8)\n",
    "df['title_tfidf'] = list(tfidf_vectorizer_titles.fit_transform(df['title']).toarray())\n",
    "\n",
    "# Sentence Embeddings for abstracts\n",
    "# df['tags_tfidf'] = list(tfidf_vectorizer.fit_transform(df['author_tags']).toarray())\n",
    "\n",
    "# Save extracted features\n",
    "df.to_pickle(\"processed_features.pkl\")\n",
    "# df['author_tags'].head()\n",
    "# df[['title_tfidf', 'tags_tfidf']].head()\n",
    "# Get the feature names (words) from the vectorizer\n",
    "vocab = tfidf_vectorizer_titles.get_feature_names_out()\n",
    "title_tfidf_vector = df['title_tfidf'][1]  # TF-IDF vector\n",
    "nonzero_indices = [i for i, value in enumerate(title_tfidf_vector) if value != 0]\n",
    "\n",
    "# Print words and their corresponding TF-IDF scores\n",
    "for index in nonzero_indices:\n",
    "    print(f\"Word: {vocab[index]}, TF-IDF Score: {title_tfidf_vector[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to specific columns (e.g., 'title' and 'abstract')\n",
    "df['title'] = df['title'].apply(preprocess_text)\n",
    "df['categories/keyword'] = df['categories/keyword'].apply(preprocess_text)\n",
    "df['author_tags'] = df['author_tags'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "df.to_csv('data/2018_cleaned_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>categories/keyword</th>\n",
       "      <th>author_tags</th>\n",
       "      <th>title_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hydrogeological parameter distribution estimat...</td>\n",
       "      <td>international journal civil engineering techno...</td>\n",
       "      <td>groundwater model hydraulic conductivity krigi...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>applying psychic distance service internationa...</td>\n",
       "      <td>journal asiapacific business</td>\n",
       "      <td>cultural distance health care service japan pe...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>estimation aloeemodin content cassia grandis c...</td>\n",
       "      <td>indian journal pharmaceutical science</td>\n",
       "      <td>aloeemodin content cassia garrettiana cassia g...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antirice pathogenic microbial activity persica...</td>\n",
       "      <td>science technology asia</td>\n",
       "      <td>antimicrobial activity essential oil persicari...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line orf protein upregulated reactive oxygen s...</td>\n",
       "      <td>cancer genomics proteomics</td>\n",
       "      <td>hne bladder cancer cancer progression immunohi...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  hydrogeological parameter distribution estimat...   \n",
       "1  applying psychic distance service internationa...   \n",
       "2  estimation aloeemodin content cassia grandis c...   \n",
       "3  antirice pathogenic microbial activity persica...   \n",
       "4  line orf protein upregulated reactive oxygen s...   \n",
       "\n",
       "                                  categories/keyword  \\\n",
       "0  international journal civil engineering techno...   \n",
       "1                       journal asiapacific business   \n",
       "2              indian journal pharmaceutical science   \n",
       "3                            science technology asia   \n",
       "4                         cancer genomics proteomics   \n",
       "\n",
       "                                         author_tags              title_tfidf  \n",
       "0  groundwater model hydraulic conductivity krigi...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "1  cultural distance health care service japan pe...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "2  aloeemodin content cassia garrettiana cassia g...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "3  antimicrobial activity essential oil persicari...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "4  hne bladder cancer cancer progression immunohi...  [0. 0. 0. ... 0. 0. 0.]  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.read_csv('data/2018_cleaned_processed.csv')\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>categories/keyword</th>\n",
       "      <th>author_tags</th>\n",
       "      <th>access</th>\n",
       "      <th>acid</th>\n",
       "      <th>activity</th>\n",
       "      <th>acute</th>\n",
       "      <th>among</th>\n",
       "      <th>analysis</th>\n",
       "      <th>application</th>\n",
       "      <th>...</th>\n",
       "      <th>thailandarticle</th>\n",
       "      <th>thailandarticleopen</th>\n",
       "      <th>therapy</th>\n",
       "      <th>treatment</th>\n",
       "      <th>two</th>\n",
       "      <th>use</th>\n",
       "      <th>using</th>\n",
       "      <th>via</th>\n",
       "      <th>virus</th>\n",
       "      <th>water</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hydrogeological parameter distribution estimat...</td>\n",
       "      <td>international journal civil engineering techno...</td>\n",
       "      <td>groundwater model hydraulic conductivity krigi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>applying psychic distance service internationa...</td>\n",
       "      <td>journal asiapacific business</td>\n",
       "      <td>cultural distance health care service japan pe...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>estimation aloeemodin content cassia grandis c...</td>\n",
       "      <td>indian journal pharmaceutical science</td>\n",
       "      <td>aloeemodin content cassia garrettiana cassia g...</td>\n",
       "      <td>0.280471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>antirice pathogenic microbial activity persica...</td>\n",
       "      <td>science technology asia</td>\n",
       "      <td>antimicrobial activity essential oil persicari...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>line orf protein upregulated reactive oxygen s...</td>\n",
       "      <td>cancer genomics proteomics</td>\n",
       "      <td>hne bladder cancer cancer progression immunohi...</td>\n",
       "      <td>0.207590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  hydrogeological parameter distribution estimat...   \n",
       "1  applying psychic distance service internationa...   \n",
       "2  estimation aloeemodin content cassia grandis c...   \n",
       "3  antirice pathogenic microbial activity persica...   \n",
       "4  line orf protein upregulated reactive oxygen s...   \n",
       "\n",
       "                                  categories/keyword  \\\n",
       "0  international journal civil engineering techno...   \n",
       "1                       journal asiapacific business   \n",
       "2              indian journal pharmaceutical science   \n",
       "3                            science technology asia   \n",
       "4                         cancer genomics proteomics   \n",
       "\n",
       "                                         author_tags    access  acid  \\\n",
       "0  groundwater model hydraulic conductivity krigi...  0.000000   0.0   \n",
       "1  cultural distance health care service japan pe...  0.000000   0.0   \n",
       "2  aloeemodin content cassia garrettiana cassia g...  0.280471   0.0   \n",
       "3  antimicrobial activity essential oil persicari...  0.000000   0.0   \n",
       "4  hne bladder cancer cancer progression immunohi...  0.207590   0.0   \n",
       "\n",
       "   activity  acute  among  analysis  application  ...  thailandarticle  \\\n",
       "0       0.0    0.0    0.0       0.0          0.0  ...         0.634454   \n",
       "1       0.0    0.0    0.0       0.0          0.0  ...         0.000000   \n",
       "2       0.0    0.0    0.0       0.0          0.0  ...         0.000000   \n",
       "3       1.0    0.0    0.0       0.0          0.0  ...         0.000000   \n",
       "4       0.0    0.0    0.0       0.0          0.0  ...         0.000000   \n",
       "\n",
       "   thailandarticleopen  therapy  treatment  two  use     using  via  virus  \\\n",
       "0                  0.0      0.0        0.0  0.0  0.0  0.000000  0.0    0.0   \n",
       "1                  0.0      0.0        0.0  0.0  0.0  0.000000  0.0    0.0   \n",
       "2                  0.0      0.0        0.0  0.0  0.0  0.540294  0.0    0.0   \n",
       "3                  0.0      0.0        0.0  0.0  0.0  0.000000  0.0    0.0   \n",
       "4                  0.0      0.0        0.0  0.0  0.0  0.000000  0.0    0.0   \n",
       "\n",
       "   water  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example pipeline for feature extraction\n",
    "# TF-IDF for titles\n",
    "tfidf_vectorizer_titles = TfidfVectorizer(max_features=10000, min_df=0.01, max_df=0.8) #exclude > 80% duplicate and < 1%\n",
    "df['title_tfidf']  = list(tfidf_vectorizer_titles.fit_transform(df['title']).toarray())\n",
    "\n",
    "X_tfidf = tfidf_vectorizer_titles.fit_transform(df['title']).toarray()\n",
    "\n",
    "# Get the feature names (the words)\n",
    "header = tfidf_vectorizer_titles.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF array to a DataFrame\n",
    "df_tfidf = pd.DataFrame(X_tfidf, columns=header)\n",
    "\n",
    "# Now you can add this DataFrame back to your original DataFrame\n",
    "df = pd.concat([df, df_tfidf], axis=1)\n",
    "df = df.drop(columns=['title_tfidf'])\n",
    "\n",
    "df.head()\n",
    "# sample_tfidf = tfidf_vectorizer_titles.fit_transform()\n",
    "\n",
    "\n",
    "# # Sentence Embeddings for abstracts\n",
    "# # df['tags_tfidf'] = list(tfidf_vectorizer.fit_transform(df['author_tags']).toarray())\n",
    "\n",
    "# # Save extracted features\n",
    "# df.to_pickle(\"processed_features.pkl\")\n",
    "# # df['author_tags'].head()\n",
    "# # df[['title_tfidf', 'tags_tfidf']].head()\n",
    "\n",
    "# print(name_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_titles = TfidfVectorizer(max_features=5000, min_df=0.01, max_df=0.8)\n",
    "title_tfidf = tfidf_vectorizer_titles.fit_transform(df['title'])\n",
    "\n",
    "# TF-IDF for tags\n",
    "tfidf_vectorizer_tags = TfidfVectorizer(max_features=5000, min_df=0.01, max_df=0.8)\n",
    "tags_tfidf = tfidf_vectorizer_tags.fit_transform(df['author_tags'])\n",
    "\n",
    "# Optionally, get the feature names (terms)\n",
    "title_feature_names = tfidf_vectorizer_titles.get_feature_names_out()\n",
    "tags_feature_names = tfidf_vectorizer_tags.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.28047099, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Combine the separate TF-IDF features into one feature matrix\n",
    "X = hstack([title_tfidf, tags_tfidf])\n",
    "\n",
    "# Convert the combined features to a dense format (optional, depending on model)\n",
    "# X = X.toarray()  # Only if your model requires dense arrays, some models like Logistic Regression do\n",
    "X.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
